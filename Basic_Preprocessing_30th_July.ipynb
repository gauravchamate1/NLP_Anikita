{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cd4e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostly used libraries in NLP\n",
    "nltk >> natural language toolkit\n",
    "gensim\n",
    "spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641043d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('package_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df80cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cabcb2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The Moon is a barren, rocky world without air and water.\n",
    "It has dark lava plain on its surface. The Moon is filled wit craters. \n",
    "It has no light of its own. It gets its light from the Sun. The Moo keeps \n",
    "changing its shape as it moves round the Earth. It spins on its axis in 27.3\n",
    "days stars were named after the Edwin Aldrin were the first ones to set their foot on the \n",
    "Moon on 21 July 1969 They reached the Moon in their space craft named Apollo II.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3111f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize,WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import contractions\n",
    "from unidecode import unidecode\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dec761c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Moon is a barren, rocky world without air and water.',\n",
       " 'It has dark lava plain on its surface.',\n",
       " 'The Moon is filled wit craters.',\n",
       " 'It has no light of its own.',\n",
       " 'It gets its light from the Sun.',\n",
       " 'The Moo keeps \\nchanging its shape as it moves round the Earth.',\n",
       " 'It spins on its axis in 27.3\\ndays stars were named after the Edwin Aldrin were the first ones to set their foot on the \\nMoon on 21 July 1969 They reached the Moon in their space craft named Apollo II.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization\n",
    "# sentece tokenization\n",
    "sent = sent_tokenize(text)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62b3304d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Moon',\n",
       " 'is',\n",
       " 'a',\n",
       " 'barren',\n",
       " ',',\n",
       " 'rocky',\n",
       " 'world',\n",
       " 'without',\n",
       " 'air',\n",
       " 'and',\n",
       " 'water',\n",
       " '.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'dark',\n",
       " 'lava',\n",
       " 'plain',\n",
       " 'on',\n",
       " 'its',\n",
       " 'surface',\n",
       " '.',\n",
       " 'The',\n",
       " 'Moon',\n",
       " 'is',\n",
       " 'filled',\n",
       " 'wit',\n",
       " 'craters',\n",
       " '.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'no',\n",
       " 'light',\n",
       " 'of',\n",
       " 'its',\n",
       " 'own',\n",
       " '.',\n",
       " 'It',\n",
       " 'gets',\n",
       " 'its',\n",
       " 'light',\n",
       " 'from',\n",
       " 'the',\n",
       " 'Sun',\n",
       " '.',\n",
       " 'The',\n",
       " 'Moo',\n",
       " 'keeps',\n",
       " 'changing',\n",
       " 'its',\n",
       " 'shape',\n",
       " 'as',\n",
       " 'it',\n",
       " 'moves',\n",
       " 'round',\n",
       " 'the',\n",
       " 'Earth',\n",
       " '.',\n",
       " 'It',\n",
       " 'spins',\n",
       " 'on',\n",
       " 'its',\n",
       " 'axis',\n",
       " 'in',\n",
       " '27.3',\n",
       " 'days',\n",
       " 'stars',\n",
       " 'were',\n",
       " 'named',\n",
       " 'after',\n",
       " 'the',\n",
       " 'Edwin',\n",
       " 'Aldrin',\n",
       " 'were',\n",
       " 'the',\n",
       " 'first',\n",
       " 'ones',\n",
       " 'to',\n",
       " 'set',\n",
       " 'their',\n",
       " 'foot',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Moon',\n",
       " 'on',\n",
       " '21',\n",
       " 'July',\n",
       " '1969',\n",
       " 'They',\n",
       " 'reached',\n",
       " 'the',\n",
       " 'Moon',\n",
       " 'in',\n",
       " 'their',\n",
       " 'space',\n",
       " 'craft',\n",
       " 'named',\n",
       " 'Apollo',\n",
       " 'II',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenization\n",
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dea71716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Moon',\n",
       " 'is',\n",
       " 'a',\n",
       " 'barren,',\n",
       " 'rocky',\n",
       " 'world',\n",
       " 'without',\n",
       " 'air',\n",
       " 'and',\n",
       " 'water.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'dark',\n",
       " 'lava',\n",
       " 'plain',\n",
       " 'on',\n",
       " 'its',\n",
       " 'surface.',\n",
       " 'The',\n",
       " 'Moon',\n",
       " 'is',\n",
       " 'filled',\n",
       " 'wit',\n",
       " 'craters.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'no',\n",
       " 'light',\n",
       " 'of',\n",
       " 'its',\n",
       " 'own.',\n",
       " 'It',\n",
       " 'gets',\n",
       " 'its',\n",
       " 'light',\n",
       " 'from',\n",
       " 'the',\n",
       " 'Sun.',\n",
       " 'The',\n",
       " 'Moo',\n",
       " 'keeps',\n",
       " 'changing',\n",
       " 'its',\n",
       " 'shape',\n",
       " 'as',\n",
       " 'it',\n",
       " 'moves',\n",
       " 'round',\n",
       " 'the',\n",
       " 'Earth.',\n",
       " 'It',\n",
       " 'spins',\n",
       " 'on',\n",
       " 'its',\n",
       " 'axis',\n",
       " 'in',\n",
       " '27.3',\n",
       " 'days',\n",
       " 'stars',\n",
       " 'were',\n",
       " 'named',\n",
       " 'after',\n",
       " 'the',\n",
       " 'Edwin',\n",
       " 'Aldrin',\n",
       " 'were',\n",
       " 'the',\n",
       " 'first',\n",
       " 'ones',\n",
       " 'to',\n",
       " 'set',\n",
       " 'their',\n",
       " 'foot',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Moon',\n",
       " 'on',\n",
       " '21',\n",
       " 'July',\n",
       " '1969',\n",
       " 'They',\n",
       " 'reached',\n",
       " 'the',\n",
       " 'Moon',\n",
       " 'in',\n",
       " 'their',\n",
       " 'space',\n",
       " 'craft',\n",
       " 'named',\n",
       " 'Apollo',\n",
       " 'II.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# whitespace tokenization\n",
    "tokens1 = WhitespaceTokenizer().tokenize(text)\n",
    "tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ceda564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'moon',\n",
       " 'is',\n",
       " 'a',\n",
       " 'barren',\n",
       " ',',\n",
       " 'rocky',\n",
       " 'world',\n",
       " 'without',\n",
       " 'air',\n",
       " 'and',\n",
       " 'water',\n",
       " '.',\n",
       " 'it',\n",
       " 'has',\n",
       " 'dark',\n",
       " 'lava',\n",
       " 'plain',\n",
       " 'on',\n",
       " 'its',\n",
       " 'surface',\n",
       " '.',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'is',\n",
       " 'filled',\n",
       " 'wit',\n",
       " 'craters',\n",
       " '.',\n",
       " 'it',\n",
       " 'has',\n",
       " 'no',\n",
       " 'light',\n",
       " 'of',\n",
       " 'its',\n",
       " 'own',\n",
       " '.',\n",
       " 'it',\n",
       " 'gets',\n",
       " 'its',\n",
       " 'light',\n",
       " 'from',\n",
       " 'the',\n",
       " 'sun',\n",
       " '.',\n",
       " 'the',\n",
       " 'moo',\n",
       " 'keeps',\n",
       " 'changing',\n",
       " 'its',\n",
       " 'shape',\n",
       " 'as',\n",
       " 'it',\n",
       " 'moves',\n",
       " 'round',\n",
       " 'the',\n",
       " 'earth',\n",
       " '.',\n",
       " 'it',\n",
       " 'spins',\n",
       " 'on',\n",
       " 'its',\n",
       " 'axis',\n",
       " 'in',\n",
       " '27.3',\n",
       " 'days',\n",
       " 'stars',\n",
       " 'were',\n",
       " 'named',\n",
       " 'after',\n",
       " 'the',\n",
       " 'edwin',\n",
       " 'aldrin',\n",
       " 'were',\n",
       " 'the',\n",
       " 'first',\n",
       " 'ones',\n",
       " 'to',\n",
       " 'set',\n",
       " 'their',\n",
       " 'foot',\n",
       " 'on',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'on',\n",
       " '21',\n",
       " 'july',\n",
       " '1969',\n",
       " 'they',\n",
       " 'reached',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'in',\n",
       " 'their',\n",
       " 'space',\n",
       " 'craft',\n",
       " 'named',\n",
       " 'apollo',\n",
       " 'ii',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalization\n",
    "lower_text = [word.lower() for word in tokens]\n",
    "lower_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d564b166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c628dc85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'moon',\n",
       " 'is',\n",
       " 'a',\n",
       " 'barren',\n",
       " 'rocky',\n",
       " 'world',\n",
       " 'without',\n",
       " 'air',\n",
       " 'and',\n",
       " 'water',\n",
       " 'it',\n",
       " 'has',\n",
       " 'dark',\n",
       " 'lava',\n",
       " 'plain',\n",
       " 'on',\n",
       " 'its',\n",
       " 'surface',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'is',\n",
       " 'filled',\n",
       " 'wit',\n",
       " 'craters',\n",
       " 'it',\n",
       " 'has',\n",
       " 'no',\n",
       " 'light',\n",
       " 'of',\n",
       " 'its',\n",
       " 'own',\n",
       " 'it',\n",
       " 'gets',\n",
       " 'its',\n",
       " 'light',\n",
       " 'from',\n",
       " 'the',\n",
       " 'sun',\n",
       " 'the',\n",
       " 'moo',\n",
       " 'keeps',\n",
       " 'changing',\n",
       " 'its',\n",
       " 'shape',\n",
       " 'as',\n",
       " 'it',\n",
       " 'moves',\n",
       " 'round',\n",
       " 'the',\n",
       " 'earth',\n",
       " 'it',\n",
       " 'spins',\n",
       " 'on',\n",
       " 'its',\n",
       " 'axis',\n",
       " 'in',\n",
       " '27.3',\n",
       " 'days',\n",
       " 'stars',\n",
       " 'were',\n",
       " 'named',\n",
       " 'after',\n",
       " 'the',\n",
       " 'edwin',\n",
       " 'aldrin',\n",
       " 'were',\n",
       " 'the',\n",
       " 'first',\n",
       " 'ones',\n",
       " 'to',\n",
       " 'set',\n",
       " 'their',\n",
       " 'foot',\n",
       " 'on',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'on',\n",
       " '21',\n",
       " 'july',\n",
       " '1969',\n",
       " 'they',\n",
       " 'reached',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'in',\n",
       " 'their',\n",
       " 'space',\n",
       " 'craft',\n",
       " 'named',\n",
       " 'apollo',\n",
       " 'ii']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove punctuations/symbols\n",
    "text_without_punct = [word for word in lower_text if word not in punctuation]\n",
    "text_without_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1de5e906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_list = stopwords.words('english')\n",
    "stopword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eded0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "803eadaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moon',\n",
       " 'barren',\n",
       " 'rocky',\n",
       " 'world',\n",
       " 'without',\n",
       " 'air',\n",
       " 'water',\n",
       " 'dark',\n",
       " 'lava',\n",
       " 'plain',\n",
       " 'surface',\n",
       " 'moon',\n",
       " 'filled',\n",
       " 'wit',\n",
       " 'craters',\n",
       " 'light',\n",
       " 'gets',\n",
       " 'light',\n",
       " 'sun',\n",
       " 'moo',\n",
       " 'keeps',\n",
       " 'changing',\n",
       " 'shape',\n",
       " 'moves',\n",
       " 'round',\n",
       " 'earth',\n",
       " 'spins',\n",
       " 'axis',\n",
       " '27.3',\n",
       " 'days',\n",
       " 'stars',\n",
       " 'named',\n",
       " 'edwin',\n",
       " 'aldrin',\n",
       " 'first',\n",
       " 'ones',\n",
       " 'set',\n",
       " 'foot',\n",
       " 'moon',\n",
       " '21',\n",
       " 'july',\n",
       " '1969',\n",
       " 'reached',\n",
       " 'moon',\n",
       " 'space',\n",
       " 'craft',\n",
       " 'named',\n",
       " 'apollo',\n",
       " 'ii']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords\n",
    "text_without_stop = [word for word in text_without_punct if word not in stopword_list]\n",
    "text_without_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4522272b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'moon', 'is', 'a', 'barren', 'rocky', 'world', 'without', 'air', 'and']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_without_punct[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a71954e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moon',\n",
       " 'barren',\n",
       " 'rocky',\n",
       " 'world',\n",
       " 'without',\n",
       " 'air',\n",
       " 'water',\n",
       " 'dark',\n",
       " 'lava',\n",
       " 'plain']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_without_stop[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48cc5895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I did not like the movie'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contraction mapping\n",
    "text1 = \"I didn't like the movie\"\n",
    "expanded_text = contractions.fix(text1)\n",
    "expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b38ce5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I does not like the movie'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contraction mapping\n",
    "text1 = \"I doesn't like the movie\"\n",
    "expanded_text = contractions.fix(text1)\n",
    "expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2426ea8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I cannot like the movie'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contraction mapping\n",
    "text1 = \"I can't like the movie\"\n",
    "expanded_text = contractions.fix(text1)\n",
    "expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cefee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "{can't:can not}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48c1233e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word >> moon\n",
      "stemmed_word >> moon\n",
      "lemmatized_word >> moon\n",
      "****************************************************************************************************\n",
      "Original word >> barren\n",
      "stemmed_word >> bar\n",
      "lemmatized_word >> barren\n",
      "****************************************************************************************************\n",
      "Original word >> rocky\n",
      "stemmed_word >> rocky\n",
      "lemmatized_word >> rocky\n",
      "****************************************************************************************************\n",
      "Original word >> world\n",
      "stemmed_word >> world\n",
      "lemmatized_word >> world\n",
      "****************************************************************************************************\n",
      "Original word >> without\n",
      "stemmed_word >> without\n",
      "lemmatized_word >> without\n",
      "****************************************************************************************************\n",
      "Original word >> air\n",
      "stemmed_word >> air\n",
      "lemmatized_word >> air\n",
      "****************************************************************************************************\n",
      "Original word >> water\n",
      "stemmed_word >> wat\n",
      "lemmatized_word >> water\n",
      "****************************************************************************************************\n",
      "Original word >> dark\n",
      "stemmed_word >> dark\n",
      "lemmatized_word >> dark\n",
      "****************************************************************************************************\n",
      "Original word >> lava\n",
      "stemmed_word >> lav\n",
      "lemmatized_word >> lava\n",
      "****************************************************************************************************\n",
      "Original word >> plain\n",
      "stemmed_word >> plain\n",
      "lemmatized_word >> plain\n",
      "****************************************************************************************************\n",
      "Original word >> surface\n",
      "stemmed_word >> surfac\n",
      "lemmatized_word >> surface\n",
      "****************************************************************************************************\n",
      "Original word >> moon\n",
      "stemmed_word >> moon\n",
      "lemmatized_word >> moon\n",
      "****************************************************************************************************\n",
      "Original word >> filled\n",
      "stemmed_word >> fil\n",
      "lemmatized_word >> filled\n",
      "****************************************************************************************************\n",
      "Original word >> wit\n",
      "stemmed_word >> wit\n",
      "lemmatized_word >> wit\n",
      "****************************************************************************************************\n",
      "Original word >> craters\n",
      "stemmed_word >> crat\n",
      "lemmatized_word >> crater\n",
      "****************************************************************************************************\n",
      "Original word >> light\n",
      "stemmed_word >> light\n",
      "lemmatized_word >> light\n",
      "****************************************************************************************************\n",
      "Original word >> gets\n",
      "stemmed_word >> get\n",
      "lemmatized_word >> get\n",
      "****************************************************************************************************\n",
      "Original word >> light\n",
      "stemmed_word >> light\n",
      "lemmatized_word >> light\n",
      "****************************************************************************************************\n",
      "Original word >> sun\n",
      "stemmed_word >> sun\n",
      "lemmatized_word >> sun\n",
      "****************************************************************************************************\n",
      "Original word >> moo\n",
      "stemmed_word >> moo\n",
      "lemmatized_word >> moo\n",
      "****************************************************************************************************\n",
      "Original word >> keeps\n",
      "stemmed_word >> keep\n",
      "lemmatized_word >> keep\n",
      "****************************************************************************************************\n",
      "Original word >> changing\n",
      "stemmed_word >> chang\n",
      "lemmatized_word >> changing\n",
      "****************************************************************************************************\n",
      "Original word >> shape\n",
      "stemmed_word >> shap\n",
      "lemmatized_word >> shape\n",
      "****************************************************************************************************\n",
      "Original word >> moves\n",
      "stemmed_word >> mov\n",
      "lemmatized_word >> move\n",
      "****************************************************************************************************\n",
      "Original word >> round\n",
      "stemmed_word >> round\n",
      "lemmatized_word >> round\n",
      "****************************************************************************************************\n",
      "Original word >> earth\n",
      "stemmed_word >> ear\n",
      "lemmatized_word >> earth\n",
      "****************************************************************************************************\n",
      "Original word >> spins\n",
      "stemmed_word >> spin\n",
      "lemmatized_word >> spin\n",
      "****************************************************************************************************\n",
      "Original word >> axis\n",
      "stemmed_word >> ax\n",
      "lemmatized_word >> axis\n",
      "****************************************************************************************************\n",
      "Original word >> 27.3\n",
      "stemmed_word >> 27.3\n",
      "lemmatized_word >> 27.3\n",
      "****************************************************************************************************\n",
      "Original word >> days\n",
      "stemmed_word >> day\n",
      "lemmatized_word >> day\n",
      "****************************************************************************************************\n",
      "Original word >> stars\n",
      "stemmed_word >> star\n",
      "lemmatized_word >> star\n",
      "****************************************************************************************************\n",
      "Original word >> named\n",
      "stemmed_word >> nam\n",
      "lemmatized_word >> named\n",
      "****************************************************************************************************\n",
      "Original word >> edwin\n",
      "stemmed_word >> edwin\n",
      "lemmatized_word >> edwin\n",
      "****************************************************************************************************\n",
      "Original word >> aldrin\n",
      "stemmed_word >> aldrin\n",
      "lemmatized_word >> aldrin\n",
      "****************************************************************************************************\n",
      "Original word >> first\n",
      "stemmed_word >> first\n",
      "lemmatized_word >> first\n",
      "****************************************************************************************************\n",
      "Original word >> ones\n",
      "stemmed_word >> on\n",
      "lemmatized_word >> one\n",
      "****************************************************************************************************\n",
      "Original word >> set\n",
      "stemmed_word >> set\n",
      "lemmatized_word >> set\n",
      "****************************************************************************************************\n",
      "Original word >> foot\n",
      "stemmed_word >> foot\n",
      "lemmatized_word >> foot\n",
      "****************************************************************************************************\n",
      "Original word >> moon\n",
      "stemmed_word >> moon\n",
      "lemmatized_word >> moon\n",
      "****************************************************************************************************\n",
      "Original word >> 21\n",
      "stemmed_word >> 21\n",
      "lemmatized_word >> 21\n",
      "****************************************************************************************************\n",
      "Original word >> july\n",
      "stemmed_word >> july\n",
      "lemmatized_word >> july\n",
      "****************************************************************************************************\n",
      "Original word >> 1969\n",
      "stemmed_word >> 1969\n",
      "lemmatized_word >> 1969\n",
      "****************************************************************************************************\n",
      "Original word >> reached\n",
      "stemmed_word >> reach\n",
      "lemmatized_word >> reached\n",
      "****************************************************************************************************\n",
      "Original word >> moon\n",
      "stemmed_word >> moon\n",
      "lemmatized_word >> moon\n",
      "****************************************************************************************************\n",
      "Original word >> space\n",
      "stemmed_word >> spac\n",
      "lemmatized_word >> space\n",
      "****************************************************************************************************\n",
      "Original word >> craft\n",
      "stemmed_word >> craft\n",
      "lemmatized_word >> craft\n",
      "****************************************************************************************************\n",
      "Original word >> named\n",
      "stemmed_word >> nam\n",
      "lemmatized_word >> named\n",
      "****************************************************************************************************\n",
      "Original word >> apollo\n",
      "stemmed_word >> apollo\n",
      "lemmatized_word >> apollo\n",
      "****************************************************************************************************\n",
      "Original word >> ii\n",
      "stemmed_word >> ii\n",
      "lemmatized_word >> ii\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# stemming and lemmatization\n",
    "stemming = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for word in text_without_stop:\n",
    "    stemmed_word = stemming.stem(word)\n",
    "    lemmatized_word = lemmatizer.lemmatize(word)\n",
    "    print(f'Original word >> {word}')\n",
    "    print(f'stemmed_word >> {stemmed_word}')\n",
    "    print(f'lemmatized_word >> {lemmatized_word}')\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92f8a114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a, A, A, e, E'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# handling accented characters\n",
    "accented_character = 'á, Á, Ä, é, È'\n",
    "fixed_words = unidecode(accented_character)\n",
    "fixed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f714bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message\n",
      "service\n"
     ]
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "spell = Speller(lang='en')\n",
    "print(spell('mussage'))\n",
    "print(spell('survice'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "433d38e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "a = 'mussage'\n",
    "b = TextBlob(a)\n",
    "print(b.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79533878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service\n"
     ]
    }
   ],
   "source": [
    "a = 'survice'\n",
    "b = TextBlob(a)\n",
    "print(b.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd9ee2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
